{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils.data_processing import _drop_consecutive_nans, add_day_ahead_column\n",
    "from utils.error_metrics import _calc_mae, _calc_mse, _calc_rmse, _calc_nrmse, _calc_mape, _calc_mase, _calc_msse, _seas_naive_fcst, _calc_metrics\n",
    "from utils.clustering import mapping_tsfeatures, clustering, sum_until_threshold, mapping_energy_metrics\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# PLOTTING\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 13  # Font size\n",
    "stanford_colors = ['#1f78b5', '#33a12c', '#e41a1c', '#ff7f00', '#6a3d9b', '#b25928', #dark\n",
    "                   '#a7cfe4', '#b3e08b', '#fc9b9a', '#fec06f', '#cbb3d7', '#ffff9a'] #light\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=stanford_colors)\n",
    "\n",
    "# ML AZURE\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import mlflow\n",
    "from config import subscription_id, resource_group, workspace_name\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "datastore = Datastore.get(workspace, \"workspaceblobstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forecast results - clustered & aggregated bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download data\n",
    "\n",
    "name_list = [f'0{j}_load_bus_after' for j in range(0,10)]\n",
    "forecast_test_dict = {}\n",
    "metrics_test_dict = {}\n",
    "\n",
    "for i, name in enumerate(name_list):\n",
    "    path = f'azureml/{name}/results/'\n",
    "    sec_char = name[1]\n",
    "    model_name = 'model ' + sec_char\n",
    "\n",
    "    print(f\"Starting {model_name}\")\n",
    "\n",
    "    file = 'forecasts_forecast_test.csv'\n",
    "    dataset = Dataset.Tabular.from_delimited_files(path=(datastore, path + file))\n",
    "    forecast_test = dataset.to_pandas_dataframe() \n",
    "    forecast_test['ds'] = pd.to_datetime(forecast_test['ds'])\n",
    "    print('IDs:', forecast_test['ID'].nunique())\n",
    "    forecast_test_dict[model_name] = forecast_test\n",
    "\n",
    "    file = 'metrics_metrics_test.csv'\n",
    "    dataset = Dataset.Tabular.from_delimited_files(path=(datastore, path + file))\n",
    "    metrics_test = dataset.to_pandas_dataframe()\n",
    "    metrics_test_dict[model_name] = metrics_test\n",
    "\n",
    "    print(f\"Finished {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add all clusters together\n",
    "\n",
    "metrics_all = pd.DataFrame()\n",
    "forecast_test_all = pd.DataFrame()\n",
    "\n",
    "for model_name, metrics_test in metrics_test_dict.items():\n",
    "    metrics_test['cluster'] = model_name\n",
    "    metrics_test['cluster'] = metrics_test['cluster'].str.extract(r'model (\\d+)').astype(int)\n",
    "\n",
    "    metrics_all = pd.concat([metrics_all, metrics_test.reset_index(drop=True)], ignore_index=True)\n",
    "    print(model_name)\n",
    "\n",
    "print('Starting forecast_test_all')\n",
    "for model_name, forecast_test in forecast_test_dict.items():\n",
    "    forecast_test['cluster'] = model_name\n",
    "    forecast_test['cluster'] = forecast_test['cluster'].str.extract(r'model (\\d+)').astype(int)\n",
    "    \n",
    "    forecast_test_all = pd.concat([forecast_test_all, forecast_test.reset_index(drop=True)], ignore_index=True)\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add yhat column\n",
    "\n",
    "forecast_test_all_da = pd.DataFrame()\n",
    "forecast_test_all_da = add_day_ahead_column(forecast_test_all, 'yhat')\n",
    "forecast_test_all_da = forecast_test_all_da.dropna(subset=['yhat_day_ahead']).reset_index(drop=True)\n",
    "forecast_test_all_da['residuals'] = forecast_test_all_da['y'] - forecast_test_all_da['yhat_day_ahead']\n",
    "forecast_test_all_da.to_csv('07_cluster_forecasts_da.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Histogram - MASE of all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(metrics_all, x=\"MASE\", nbins=500, marginal=\"box\", title='Distribution of MASE')\n",
    "fig.update_layout(xaxis_title='MASE', yaxis_title='Count')\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Error on country-level (aggregated for comparison with country-level forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_test_all_da = pd.read_csv('07_cluster_forecasts_da.csv')\n",
    "forecast_test_all_da['ID'] = forecast_test_all_da['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add country column\n",
    "\n",
    "forecast_test_all_da['country'] = forecast_test_all_da['ID'].apply(lambda x: x.split('_')[0] if '_' in x else None)\n",
    "\n",
    "mapping = pd.read_csv('02_mapping.csv')\n",
    "mapping['ID'] = mapping['ID'].astype(str)\n",
    "mapping['ID'] = mapping['ID'].astype(str)\n",
    "temp = mapping[['ID', 'country']]\n",
    "merged_df = forecast_test_all_da.merge(temp, on='ID', how='left')\n",
    "forecast_test_all_da['country'] = merged_df['country_y'].fillna(merged_df['country_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate metrics for each country\n",
    "\n",
    "forecast_test_country_agg = pd.DataFrame()\n",
    "metrics_country = pd.DataFrame()\n",
    "temp = forecast_test_all_da[['ds', 'y', 'ID', 'yhat_day_ahead', 'country']]\n",
    "\n",
    "list = temp['country'].unique()\n",
    "\n",
    "for country in list:\n",
    "    string = f'{country}'\n",
    "\n",
    "    filtered_rows_country = temp[temp['country'] == string]\n",
    "\n",
    "    filtered_rows_country = filtered_rows_country.groupby(['ds']).sum().reset_index()\n",
    "    filtered_rows_country['ID'] = string\n",
    "\n",
    "    mae = _calc_mae(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'])\n",
    "    mse = _calc_mse(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'])\n",
    "    rmse = _calc_rmse(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'])\n",
    "    mape = _calc_mape(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'])\n",
    "\n",
    "    filtered_rows_country['snaive'] = filtered_rows_country['y'].shift(48)\n",
    "\n",
    "    mase = _calc_mase(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'], snaive_predictions=filtered_rows_country['snaive'])\n",
    "    msse = _calc_msse(predictions=filtered_rows_country['yhat_day_ahead'], truth=filtered_rows_country['y'], snaive_predictions=filtered_rows_country['snaive'])\n",
    "\n",
    "    new_row = {'ID':string, 'RMSE':rmse, 'MAE':mae, 'MAPE':mape, 'MASE':mase, 'MSSE':msse}\n",
    "    metrics_country = pd.concat([metrics_country, pd.DataFrame([new_row])])\n",
    "    \n",
    "    forecast_test_country_agg = pd.concat([forecast_test_country_agg, filtered_rows_country], ignore_index=True)\n",
    "\n",
    "## Average error metrics for all countries\n",
    "metrics_country_avg = pd.DataFrame([{'RMSE': metrics_country['RMSE'].mean(), 'MAE': metrics_country['MAE'].mean(), 'MAPE': metrics_country['MAPE'].mean(), 'MASE': metrics_country['MASE'].mean(), 'MSSE': metrics_country['MSSE'].mean()}])\n",
    "print('Average metrics for all Countries:')\n",
    "metrics_country_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_knn = pd.read_csv('../01_Benchmarks/metrics_knn.csv')\n",
    "metrics_arima = pd.read_csv('../01_Benchmarks/metrics_arima.csv')\n",
    "metrics_xgboost = pd.read_csv('../01_Benchmarks/metrics_xgboost.csv')\n",
    "metrics_snaive = pd.read_csv('../01_Benchmarks/metrics_snaive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot distribution of two error metrics for every model in two plots\n",
    "\n",
    "error1 = 'MAE'\n",
    "error2 = 'MASE'\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 6), layout=\"constrained\")\n",
    "\n",
    "colors = stanford_colors[0:4]\n",
    "\n",
    "bp1 = ax[0].boxplot([metrics_country[error1], metrics_knn[error1], metrics_arima[error1], metrics_xgboost[error1]], vert=False, labels=['Proposed\\nbus model', 'k-NN', 'ARIMA', 'XGBoost'], showfliers=False)\n",
    "ax[0].set_xlabel(f'{error1} [MW]')\n",
    "\n",
    "bp2 = ax[1].boxplot([metrics_country[error2], metrics_knn[error2], metrics_arima[error2], metrics_xgboost[error2]], vert=False, labels=['Proposed\\nbus model', 'k-NN', 'ARIMA', 'XGBoost'], showfliers=False)\n",
    "ax[1].set_xlabel(f'{error2} [%]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Country plot - bus\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "europe = world[world.continent == 'Europe']\n",
    "europe.loc[europe['iso_a3'] == 'PRT', 'iso_a3'] = 'POR'\n",
    "europe_merged = europe.merge(metrics_country, how='left', left_on='iso_a3', right_on='ID')\n",
    "\n",
    "# Plot\n",
    "color_country = 'lightgray'\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "world.plot(ax=ax[0], color=color_country, edgecolor=color_country)\n",
    "europe_merged_plot1 = europe_merged.plot(column='MASE', cmap='YlOrRd', linewidth=1.0, ax=ax[0], edgecolor=color_country, legend=False, vmin=0.2, vmax=1)\n",
    "\n",
    "world.plot(ax=ax[1], color=color_country, edgecolor=color_country)\n",
    "europe_merged_plot2 = europe_merged.plot(column='MAE', cmap='YlOrRd', linewidth=1.0, ax=ax[1], edgecolor=color_country, legend=False, vmin=0, vmax=3000)\n",
    "\n",
    "ax[0].set_ylim(33, 60)\n",
    "ax[0].set_xlim(-10, 30)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "ax[1].set_ylim(33, 60)\n",
    "ax[1].set_xlim(-10, 30)\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "cbar1 = fig.colorbar(europe_merged_plot1.get_children()[1], ax=ax[0], orientation='horizontal', pad=0.03)\n",
    "cbar1.set_label('MASE [%]')\n",
    "\n",
    "cbar2 = fig.colorbar(europe_merged_plot2.get_children()[1], ax=ax[1], orientation='horizontal', pad=0.03)\n",
    "cbar2.set_label('MAE [MW]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cluster validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare mapping\n",
    "\n",
    "mapping = pd.read_csv(\"02_mapping.csv\", index_col=0)\n",
    "mapping['new_ID'] = mapping['country'].astype(str) + '_cluster0' + mapping['cluster'].astype(str) + '_agg_no_' + mapping['aggregation_no'].astype(str)\n",
    "mapping = mapping.rename(columns={'cluster': 'cluster_1'})\n",
    "\n",
    "cluster_mapping = forecast_test_all_da.drop_duplicates(subset=['ID', 'cluster']).reset_index(drop=True)\n",
    "cluster_mapping = cluster_mapping[['ID', 'cluster']]\n",
    "temp = cluster_mapping.rename(columns={'ID': 'new_ID'})\n",
    "mapping = mapping.merge(temp, on='new_ID', how='left')\n",
    "mapping = mapping.rename(columns={'cluster': 'cluster_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate magnitude of each node based on train data - Output [ID, y_mean, relative_magnitude] - 1min\n",
    "\n",
    "buss_train = pd.read_csv(\"01_load_bus.csv\")\n",
    "buss_train['ds'] = pd.to_datetime(buss_train['ds'])\n",
    "buss_train['ID'] = buss_train['ID'].astype(str)\n",
    "print(\"loaded train data\")\n",
    "\n",
    "buss_train = buss_train[buss_train['ds'] < '2014-01-01']\n",
    "temp = buss_train[['ID', 'y']]\n",
    "buss_train_mean = temp.groupby('ID').mean().reset_index()\n",
    "buss_train_sum = buss_train_mean['y'].sum()\n",
    "buss_train_mean['relative_magnitude'] = buss_train_mean['y'] / buss_train_sum\n",
    "buss_train_mean = buss_train_mean.rename(columns={'y': 'y_mean'})\n",
    "buss_train_mean['y_mean'] = buss_train_mean['y_mean'].abs()\n",
    "buss_magnitude = buss_train_mean\n",
    "buss_magnitude['ID'] = buss_magnitude['ID'].astype(str)\n",
    "\n",
    "### Calculate the magnitude after aggregation - Output [new_ID, relative_magnitude_agg]\n",
    "buss_magnitude_aggregated = pd.DataFrame()\n",
    "helper = []\n",
    "\n",
    "for index, row in mapping.iterrows():\n",
    "    ids = row['ID_list'].split(';')\n",
    "    new_id = row['new_ID']\n",
    "\n",
    "    filtered_rows = buss_magnitude[buss_magnitude['ID'].isin(ids)]\n",
    "    magnitude = filtered_rows['relative_magnitude'].sum()\n",
    "    tempo = pd.DataFrame({'new_ID': [new_id], 'relative_magnitude_agg': [magnitude]})\n",
    "    helper.append(tempo)\n",
    "\n",
    "buss_magnitude_aggregated = pd.concat(helper).reset_index(drop=True)\n",
    "\n",
    "temp = forecast_test_all_da[~forecast_test_all_da['ID'].isin(buss_magnitude_aggregated['new_ID'])]\n",
    "temp = temp['ID'].unique()\n",
    "temp = buss_magnitude[buss_magnitude['ID'].isin(temp)]\n",
    "not_agg = temp[['ID', 'relative_magnitude']]\n",
    "buss_magnitude_aggregated = pd.concat([buss_magnitude_aggregated, not_agg.rename(columns={'ID': 'new_ID', 'relative_magnitude': 'relative_magnitude_agg'})]).reset_index(drop=True)\n",
    "\n",
    "### Calculate the magnitude for each cluster - Output [cluster, relative_magnitude_cluster]\n",
    "buss_magnitude_cluster = buss_magnitude_aggregated.merge(cluster_mapping.rename(columns={'ID': 'new_ID'}), on='new_ID', how='left')\n",
    "buss_magnitude_cluster = buss_magnitude_cluster.drop(columns=['new_ID'])\n",
    "buss_magnitude_cluster = buss_magnitude_cluster.groupby('cluster').sum().reset_index().rename(columns={'relative_magnitude_agg': 'relative_magnitude_cluster'})\n",
    "buss_magnitude_cluster = buss_magnitude_cluster.sort_values(by='relative_magnitude_cluster', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate tsfeatures & energy metrics (em) - using tsfeatures package - might take a while\n",
    "\n",
    "list = []\n",
    "del list\n",
    "\n",
    "temp = forecast_test_all_da[['ds', 'y', 'ID']]\n",
    "temp['y'] = temp['y'].astype('float32')\n",
    "\n",
    "## TSFeatures & absolute em\n",
    "ID_emp_features_copy_ts = mapping_tsfeatures(df=temp, normalise=True, calulate_tsfeatures=True, calculate_em=True)\n",
    "ID_emp_features_copy_ts = ID_emp_features_copy_ts.iloc[:, [0] + [1] + list(range(13, 27))]\n",
    "\n",
    "## Relative em\n",
    "ID_emp_features_copy_em = mapping_energy_metrics(df=temp, normalise=True)\n",
    "ID_emp_features_copy_em = ID_emp_features_copy_em.iloc[:, [0] + list(range(14, 20))]\n",
    "\n",
    "## Merge\n",
    "ID_emp_features_copy = ID_emp_features_copy_ts.merge(ID_emp_features_copy_em, on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Further processing of features\n",
    "\n",
    "ID_emp_features = ID_emp_features_copy.copy()\n",
    "\n",
    "temp = forecast_test_all_da[['ID', 'cluster']]\n",
    "temp = temp.drop_duplicates(subset=['ID', 'cluster']).reset_index(drop=True)\n",
    "\n",
    "tempo = pd.DataFrame(columns=['cluster', 'size'])\n",
    "tempo['size'] = temp.groupby('cluster').size().reset_index(drop=True)\n",
    "tempo['cluster'] = tempo.index\n",
    "\n",
    "magnitude = buss_magnitude_cluster\n",
    "magnitude = magnitude.merge(tempo, on='cluster', how='left')\n",
    "magnitude['Average Load Share'] = magnitude['relative_magnitude_cluster'] / magnitude['size']\n",
    "\n",
    "ID_emp_features = cluster_mapping.merge(ID_emp_features, on='ID', how='left')\n",
    "ID_emp_features = magnitude[['cluster', 'relative_magnitude_cluster', 'Average Load Share']].merge(ID_emp_features, on='cluster', how='left')\n",
    "ID_emp_features = ID_emp_features[ ['ID'] + [ col for col in ID_emp_features.columns if col != 'ID' ] ]\n",
    "\n",
    "## Scaling of Features - choose between Standardization, Min/Max Scaling, Percentile Rank\n",
    "scale = 'percentile_rank'\n",
    "\n",
    "if scale == 'standardize':\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(ID_emp_features.iloc[:, 3:])\n",
    "    ID_emp_features.iloc[:, 3:] = scaler.transform(ID_emp_features.iloc[:, 3:])\n",
    "\n",
    "if scale == 'percentile_rank':\n",
    "    ID_emp_features.iloc[:, 2:] = ID_emp_features.iloc[:, 2:].rank(pct=True, method='min')\n",
    "\n",
    "if scale == 'min_max':\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(ID_emp_features.iloc[:, 3:])\n",
    "    ID_emp_features.iloc[:, 3:] = scaler.transform(ID_emp_features.iloc[:, 3:])\n",
    "\n",
    "## Renaming\n",
    "median_cluster_emp = ID_emp_features.drop(columns=['ID'])\n",
    "median_cluster_emp = median_cluster_emp.groupby('cluster').median().reset_index()\n",
    "median_cluster_emp = median_cluster_emp.drop(columns=['cluster'])\n",
    "\n",
    "median_cluster_emp.index = median_cluster_emp.index.astype(str)\n",
    "median_cluster_emp.index = ['Cluster ' + cluster.astype(str) + f'  #IDs: {len(temp[temp[\"cluster\"] == cluster])}' for cluster in temp['cluster'].unique()]\n",
    "median_cluster_emp = median_cluster_emp.drop(columns=['var', 'nperiods', 'seasonal_period'])\n",
    "\n",
    "median_cluster_emp = median_cluster_emp.sort_values(by='relative_magnitude_cluster', ascending=False)\n",
    "median_cluster_emp = median_cluster_emp.rename(columns={'relative_magnitude_cluster': 'Overall Load Share'})\n",
    "\n",
    "median_cluster_emp = median_cluster_emp.rename(columns={\n",
    "    'lumpiness': 'Lumpiness',\n",
    "    'stability': 'Stability',\n",
    "    'entropy': 'Entropy',\n",
    "    'trend': 'Trend',\n",
    "    'spike': 'Spike',\n",
    "    'linearity': 'Linearity',\n",
    "    'curvature': 'Curvature',\n",
    "    'e_acf1': 'ACF1',\n",
    "    'e_acf10': 'ACF10',\n",
    "    'seasonal_strength': 'Seasonal Strength',\n",
    "    'peak': 'Peak',\n",
    "    'trough': 'Troughs',\n",
    "    'rbase': '$r_{base}$',\n",
    "    'rminmax': '$r_{minmax}$',\n",
    "    'rm2w': '$r_{m2w}$',\n",
    "    'rn2w': '$r_{n2w}$',\n",
    "    're2w': '$r_{e2w}$',\n",
    "    'rni2w': '$r_{ni2w}$',\n",
    "    })\n",
    "\n",
    "median_cluster_emp = median_cluster_emp.T\n",
    "\n",
    "## Late ordering after Average Load Share - Comment put to order after Total Load Share\n",
    "average_load_share_row = median_cluster_emp.loc['Average Load Share']\n",
    "sorted_columns = average_load_share_row.sort_values(ascending=False).index\n",
    "median_cluster_emp = median_cluster_emp[sorted_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for subplot 0 - error of individual buses per cluster\n",
    "\n",
    "metrics_bus_cluster = pd.DataFrame()\n",
    "temp = forecast_test_all_da[['ds', 'y', 'ID', 'yhat_day_ahead', 'cluster']]\n",
    "\n",
    "for cluster in temp['cluster'].unique():\n",
    "    filtered_rows_cluster = temp[temp['cluster'] == cluster]\n",
    "\n",
    "    for id in filtered_rows_cluster['ID'].unique():\n",
    "        filtered_rows = filtered_rows_cluster[filtered_rows_cluster['ID'] == id]\n",
    "\n",
    "        mae = _calc_mae(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'])\n",
    "        mse = _calc_mse(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'])\n",
    "        rmse = _calc_rmse(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'])\n",
    "        mape = _calc_mape(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'])\n",
    "\n",
    "        filtered_rows['snaive'] = filtered_rows['y'].shift(48)\n",
    "\n",
    "        mase = _calc_mase(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'], snaive_predictions=filtered_rows['snaive'])\n",
    "        msse = _calc_msse(predictions=filtered_rows['yhat_day_ahead'], truth=filtered_rows['y'], snaive_predictions=filtered_rows['snaive'])\n",
    "\n",
    "        new_row = {'cluster': cluster, 'ID':id, 'RMSE':rmse, 'MAE':mae, 'MAPE':mape, 'MASE':mase, 'MSSE':msse}\n",
    "        metrics_bus_cluster = pd.concat([metrics_bus_cluster, pd.DataFrame([new_row])])\n",
    "\n",
    "metrics_bus_cluster = metrics_bus_cluster.merge(buss_magnitude_cluster, how='left', on='cluster')\n",
    "\n",
    "temp = pd.DataFrame(columns=['cluster', 'size'])\n",
    "temp['size'] = metrics_bus_cluster.groupby('cluster').size().reset_index(drop=True)\n",
    "temp['cluster'] = temp.index\n",
    "metrics_bus_cluster = metrics_bus_cluster.merge(temp, on='cluster', how='right')\n",
    "metrics_bus_cluster['Average Load Share'] = metrics_bus_cluster['relative_magnitude_cluster'] / metrics_bus_cluster['size']\n",
    "metrics_bus_cluster = metrics_bus_cluster.drop(columns=['size'])\n",
    "\n",
    "grouped_data = metrics_bus_cluster.groupby('cluster')['MAE'].apply(list).reset_index(name='MAE')\n",
    "grouped_data = grouped_data.merge(metrics_bus_cluster[['cluster', 'Average Load Share']], on='cluster', how='left')\n",
    "grouped_data = grouped_data.sort_values(by='Average Load Share', ascending=False).reset_index(drop=True)\n",
    "grouped_data = grouped_data.drop_duplicates(subset='cluster', kebuss='first').reset_index(drop=True)\n",
    "\n",
    "mase_values = [values for values in grouped_data['MAE']]\n",
    "cluster_labels = [cluster for cluster in grouped_data['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10, 11), sharex=True, gridspec_kw={'height_ratios': [6, len(median_cluster_emp.iloc[:2].index), len(median_cluster_emp.iloc[2:14].index), len(median_cluster_emp.iloc[14:].index)]})\n",
    "\n",
    "## Subplot 0 - Error Distribution\n",
    "positions = range(len(cluster_labels))\n",
    "positions = [pos + 0.5 for pos in positions]\n",
    "axs[0].boxplot(mase_values, labels=cluster_labels, positions=positions, showfliers=False)\n",
    "axs[0].set_title('Distribution of MAE Error Metric on Bus-Level by Cluster')\n",
    "axs[0].set_ylabel('MAE [MW]', rotation=0, labelpad=45)\n",
    "\n",
    "## Subplot 1 - Magnitude\n",
    "heatmap1 = sns.heatmap(median_cluster_emp.iloc[:2], cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=.5, annot_kws={\"fontsize\":8}, ax=axs[1], cbar=False)\n",
    "axs[1].set_title('Load Share')\n",
    "yticks_positions = [tick + 0.5 for tick in range(len(median_cluster_emp.iloc[:2].index))]  # Calculate middle of each row\n",
    "heatmap1.set_yticks(yticks_positions)\n",
    "heatmap1.set_yticklabels(median_cluster_emp.iloc[:2].index, rotation=0)\n",
    "\n",
    "## Subplot 2 - TS\n",
    "heatmap2 = sns.heatmap(median_cluster_emp.iloc[2:14], cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=.5, annot_kws={\"fontsize\":8}, ax=axs[2], cbar=False)\n",
    "axs[2].set_title('Time Series Characteristics')\n",
    "\n",
    "## Subplot 3 - Energy\n",
    "heatmap3 = sns.heatmap(median_cluster_emp.iloc[14:], cmap='coolwarm', annot=False, fmt=\".2f\", linewidths=.5, annot_kws={\"fontsize\":8}, ax=axs[3], cbar=False)\n",
    "axs[3].set_title('Relative Energy Metrics')\n",
    "\n",
    "## Add colorbar\n",
    "cbar_ax = fig.add_axes([1.0, 0.45, 0.01, 0.3])   # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(heatmap2.collections[0], cax=cbar_ax)\n",
    "cbar.set_label('Median Values of Percentile-Ranked Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Error plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Residual bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing the inputs\n",
    "\n",
    "## Top cluster to show - change for no. of cluster\n",
    "no_cluster = 5\n",
    "\n",
    "## Aggregation per Cluster - Whether to aggregate the data per cluster and timestamp or not\n",
    "aggregation_per_cluster = False\n",
    "\n",
    "## Tme stamp - maybe adjust\n",
    "start_date = pd.to_datetime('2014-08-11')\n",
    "end_date = start_date + pd.Timedelta(days=7)\n",
    "forecast_test_all_da['ds'] = pd.to_datetime(forecast_test_all_da['ds'])\n",
    "temp_df = forecast_test_all_da[(forecast_test_all_da['ds'] >= start_date) & (forecast_test_all_da['ds'] <= end_date)]\n",
    "temp_df = temp_df[['ds', 'ID', 'y', 'yhat_day_ahead', 'residuals']]\n",
    "\n",
    "temp_df['MAE'] = temp_df['residuals'].abs()\n",
    "temp_df_agg = temp_df.copy()\n",
    "temp_df_agg['residual_total'] = temp_df_agg['y'] - temp_df_agg['yhat_day_ahead']\n",
    "temp_df_agg = temp_df_agg.drop(columns=['ID'])\n",
    "temp_df_agg = temp_df_agg.groupby('ds').sum().reset_index()\n",
    "temp_df = temp_df.merge(temp_df_agg[['ds', 'residual_total']], on='ds', how='left')\n",
    "temp_df = temp_df.merge(cluster_mapping, on='ID', how='left')\n",
    "temp_df = temp_df.merge(buss_magnitude_aggregated.rename(columns={'new_ID': 'ID'}), on='ID', how='left')\n",
    "magnitude_sort = temp_df.groupby('cluster').agg({'relative_magnitude_agg': 'sum'}).sort_values(by='relative_magnitude_agg', ascending=False).reset_index()\n",
    "top_cluster = magnitude_sort['cluster'].head(no_cluster).tolist()\n",
    "\n",
    "## Aggregation per Cluster\n",
    "if aggregation_per_cluster:\n",
    "    temp_df = temp_df.drop(columns=['ID'])\n",
    "    temp_df = temp_df.groupby(['ds', 'cluster']).sum().reset_index()\n",
    "    temp_df = temp_df.sort_values(by='relative_magnitude_agg', ascending=False)\n",
    "\n",
    "top_df = temp_df[temp_df['cluster'].isin(top_cluster)]\n",
    "low_df = temp_df[~temp_df['cluster'].isin(top_cluster)]\n",
    "low_df['cluster'] = np.where(low_df['residuals'] > 0, 'Other Positive', 'Other Negative')\n",
    "temp_df = pd.concat([top_df, low_df], ignore_index=True)\n",
    "grouped = temp_df.groupby(['ds', 'cluster']).residuals.sum().reset_index()\n",
    "pivot_df = grouped.pivot(index='ds', columns='cluster', values='residuals').fillna(0)\n",
    "data = pivot_df.values.T\n",
    "data = data.astype(float)\n",
    "data_shape = np.shape(data)\n",
    "\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d\n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "colors = stanford_colors[:no_cluster] + ['lightgray', 'lightgray']\n",
    "pivot_df.columns = pivot_df.columns.astype(str)\n",
    "pivot_df.columns = ['Cluster ' + col if col.isdigit() else col for col in pivot_df.columns]\n",
    "component_labels = pivot_df.columns.tolist()\n",
    "component_labels = ['Other' if x in ['Other Positive', 'Other Negative'] else x for x in component_labels]\n",
    "\n",
    "## Bars\n",
    "handles = []\n",
    "labels = []\n",
    "for i, label in enumerate(pivot_df.columns):\n",
    "    bar = ax.bar(pivot_df.index, data[i], bottom=data_stack[i], width=0.03, label=component_labels[i], color=colors[i])\n",
    "    handles.append(bar[0])\n",
    "    labels.append(component_labels[i])\n",
    "\n",
    "## Residual line\n",
    "ax.plot(pivot_df.index, pivot_df.sum(axis=1), color='black', linewidth=2, label='Total')\n",
    "handles.insert(0, ax.lines[0])\n",
    "labels.insert(0, 'Total')\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq='1H')\n",
    "custom_ticks = pd.date_range(start_date, end_date, freq='12H')\n",
    "custom_labels = []\n",
    "\n",
    "for tick in custom_ticks:\n",
    "    if tick.hour == 0:\n",
    "        custom_labels.append(tick.strftime('%a'))\n",
    "    else:\n",
    "        custom_labels.append(tick.strftime('%H:%M'))\n",
    "\n",
    "ax.set_xticks(custom_ticks)\n",
    "ax.set_xticklabels(custom_labels)\n",
    "ax.legend(handles = handles[:-1], labels = labels[:-1], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.set_ylabel('Residuals [MW]')\n",
    "ax.set_xlabel('Date')\n",
    "fig.show()\n",
    "\n",
    "plt.rcParams['font.size'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing Residuals\n",
    "\n",
    "residuals = forecast_test_all_da[['ds', 'cluster', 'y', 'yhat_day_ahead', 'residuals']]\n",
    "residuals = residuals.groupby(['ds', 'cluster']).sum().reset_index()\n",
    "residuals = residuals[residuals['cluster'].isin(top_cluster)]\n",
    "residuals['ds'] = pd.to_datetime(residuals['ds'])\n",
    "residuals = residuals[(residuals['ds'] >= start_date) & (residuals['ds'] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "ax1 = axes[0]\n",
    "colors = stanford_colors[:no_cluster] + ['lightgray', 'lightgray']\n",
    "pivot_df.columns = pivot_df.columns.astype(str)\n",
    "pivot_df.columns = ['Cluster ' + col if col.isdigit() else col for col in pivot_df.columns]\n",
    "component_labels = pivot_df.columns.tolist()\n",
    "component_labels = ['Other' if x in ['Other Positive', 'Other Negative'] else x for x in component_labels]\n",
    "\n",
    "## Bars\n",
    "handles = []\n",
    "labels = []\n",
    "for i, label in enumerate(pivot_df.columns):\n",
    "    bar = ax1.bar(pivot_df.index, data[i], bottom=data_stack[i], width=0.03, label=component_labels[i], color=colors[i])\n",
    "    handles.append(bar[0])\n",
    "    labels.append(component_labels[i])\n",
    "\n",
    "## Residual line\n",
    "ax1.plot(pivot_df.index, pivot_df.sum(axis=1), color='black', linewidth=2, label='Total')\n",
    "handles.insert(0, ax1.lines[0])\n",
    "labels.insert(0, 'Total')\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq='1H')\n",
    "custom_ticks = pd.date_range(start_date, end_date, freq='12H')\n",
    "custom_labels = []\n",
    "\n",
    "for tick in custom_ticks:\n",
    "    if tick.hour == 0:\n",
    "        custom_labels.append(tick.strftime('%a'))\n",
    "    else:\n",
    "        custom_labels.append(tick.strftime('%H:%M'))\n",
    "\n",
    "ax1.set_xticks(custom_ticks)\n",
    "ax1.set_xticklabels(custom_labels)\n",
    "ax1.axhline(0, color='black', linestyle='--')\n",
    "ax1.legend(handles = handles[:-1], labels = labels[:-1], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax1.set_ylabel('Residuals [MW]')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2 = plt.gca()\n",
    "temp = residuals['cluster'].unique()\n",
    "\n",
    "for cluster in temp:\n",
    "    data = residuals[residuals['cluster'] == cluster]\n",
    "    ax2.plot(data['ds'], data['residuals'], linewidth=1, label=f'Cluster {cluster}')\n",
    "\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Residuals [MW]')\n",
    "ax2.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Error share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting preparation\n",
    "\n",
    "temp_save = forecast_test_all_da[['ds', 'ID', 'y', 'yhat_day_ahead', 'residuals']]\n",
    "temp_save = temp_save.merge(cluster_mapping, on='ID', how='left')\n",
    "temp_save = temp_save.drop(columns=['ID'])\n",
    "temp_save = temp_save.groupby(['ds', 'cluster']).sum().reset_index()\n",
    "\n",
    "temp_save['residuals'] = temp_save['y'] - temp_save['yhat_day_ahead']\n",
    "temp_save['MAE'] = temp_save['residuals'].abs()\n",
    "\n",
    "## MAE Share - Subplot 1\n",
    "total_error_mae = temp_save[['cluster', 'MAE']].groupby('cluster').mean().reset_index()\n",
    "total_mae = total_error_mae['MAE'].sum()\n",
    "total_error_mae['MAE_share'] = total_error_mae['MAE'] / total_mae\n",
    "\n",
    "## Total Residual per Timestamp - Subplots 2 & 3\n",
    "temp_df_agg = temp_save.copy()\n",
    "temp_df_agg = temp_df_agg.groupby(['ds', 'cluster']).sum().reset_index()\n",
    "temp_df_agg['residual_total'] = temp_df_agg['y'] - temp_df_agg['yhat_day_ahead']\n",
    "temp_df_agg = temp_df_agg.groupby('ds').sum().reset_index()\n",
    "temp_save = temp_save.merge(temp_df_agg[['ds', 'residual_total']], on='ds', how='left')\n",
    "\n",
    "## Magnitude - Subplot 4\n",
    "total_error_magnitude = temp_save[['cluster', 'y']].groupby('cluster').mean().reset_index()\n",
    "total_magnitude = total_error_magnitude['y'].sum()\n",
    "total_error_magnitude['magnitude_share'] = total_error_magnitude['y'] / total_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparation\n",
    "\n",
    "quantile = 0.90\n",
    "\n",
    "## Positive Share\n",
    "result_df_pos = temp_save[temp_save['residual_total'] > 0]\n",
    "threshold = result_df_pos['residual_total'].quantile(quantile)\n",
    "result_df_pos = result_df_pos[result_df_pos['residual_total'] > threshold]\n",
    "total_error_pos = result_df_pos.groupby('cluster').mean().reset_index()\n",
    "total_error_pos['bias_pos'] = total_error_pos.apply(lambda row: row['residuals'] / row['residual_total'], axis=1)\n",
    "total_error_pos['error_pos'] = total_error_pos.apply(lambda row: row['MAE'] / row['residual_total'], axis=1)\n",
    "\n",
    "## Negative Share\n",
    "temp_save['residual_total'] = temp_save['residual_total'] * -1\n",
    "result_df_neg = temp_save[temp_save['residual_total'] > 0]\n",
    "threshold = result_df_neg['residual_total'].quantile(quantile)\n",
    "result_df_neg = result_df_neg[result_df_neg['residual_total'] > threshold]\n",
    "temp_save['residual_total'] = temp_save['residual_total'] * -1\n",
    "total_error_neg = result_df_neg.groupby('cluster').mean().reset_index()\n",
    "total_error_neg['bias_neg'] = total_error_neg.apply(lambda row: row['residuals'] / row['residual_total'], axis=1)\n",
    "total_error_neg['error_neg'] = total_error_neg.apply(lambda row: row['MAE'] / row['residual_total'], axis=1)\n",
    "\n",
    "total_error = total_error_pos[['cluster', 'bias_pos', 'error_pos']].merge(total_error_neg[['cluster', 'bias_neg', 'error_neg']], on='cluster', how='left')\n",
    "total_error = total_error.merge(total_error_mae[['cluster', 'MAE_share']], on='cluster', how='left')\n",
    "total_error = total_error.merge(total_error_magnitude[['cluster', 'magnitude_share']], on='cluster', how='left')\n",
    "total_error['cluster_names'] = total_error['cluster'].apply(lambda x: f'Cluster {x}')\n",
    "total_error = total_error.sort_values(by='magnitude_share', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "bias = 'black'\n",
    "error = 'lightgray'\n",
    "points = 'gray'\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "gs = gridspec.GridSpec(1, 4, width_ratios=[0.5, 1, 1, 0.5], wspace=0.45)\n",
    "\n",
    "## Subplot 1: horizontal bar plot with relative share of RES\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax1.barh(total_error['cluster_names'], total_error['MAE_share'], color='none', edgecolor=points, label='Relative Aggregated Residuals')\n",
    "ax1.set_xlabel('Overall MAE Share')\n",
    "ax1.set_yticks([])\n",
    "ax1.tick_params(axis=u'both', which=u'both', length=0)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "## Subplot 2: Vertical line at x=0 and negative bar plot\n",
    "ax2 = plt.subplot(gs[1])\n",
    "ax2.barh(total_error['cluster_names'], total_error['error_neg'], label='Mean Absolute Error', color=error)\n",
    "ax2.barh(total_error['cluster_names'], abs(total_error['bias_neg']), label='Mean Error (Bias)', color=bias)\n",
    "ax2.set_xlabel(f'Relative Contribution to {round(100-quantile*100)} Percent \\n Highest Negative Residuals')\n",
    "ax2.set_yticks([])\n",
    "ax2.invert_xaxis()\n",
    "ax2.grid(True, axis='x', color='lightgray')\n",
    "ax2.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "## Subplot 3: Horizontal bar plot with positive RES\n",
    "ax3 = plt.subplot(gs[2])\n",
    "ax3.barh(total_error['cluster_names'], total_error['error_pos'], label='Mean Absolute Error', color=error)\n",
    "ax3.barh(total_error['cluster_names'], total_error['bias_pos'], label='Mean Error (Bias)', color=bias)\n",
    "ax3.set_xlabel(f'Relative Contribution to {round(100-quantile*100)} Percent \\n Highest Positive Residuals')\n",
    "ax3.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax3.grid(True, axis='x', color='lightgray')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['bottom'].set_visible(False)\n",
    "ax3.spines['left'].set_visible(False)\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "## Subplot 4: Horizontal bar plot with magnitude\n",
    "ax4 = plt.subplot(gs[3])\n",
    "ax4.barh(total_error['cluster_names'], total_error['magnitude_share'], color='none', edgecolor=points, label='Relative Magnitude')\n",
    "ax4.set_xlabel('Overall Load Share')\n",
    "ax4.set_yticks([])\n",
    "ax4.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax4.spines['top'].set_visible(False)\n",
    "ax4.spines['right'].set_visible(False)\n",
    "ax4.spines['bottom'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Residuals in one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose how many clusters to show\n",
    "cluster_to_show = 5\n",
    "\n",
    "forecast_one_cluster = forecast_test_all_da[forecast_test_all_da['cluster'] == cluster_to_show]\n",
    "print('Number of buses/nodes in cluster:', forecast_one_cluster['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparation\n",
    "\n",
    "## Top buses to show\n",
    "no_id = 10\n",
    "\n",
    "## Aggregation per Cluster - Whether to aggregate the data per cluster and timestamp or not\n",
    "aggregation_per_cluster = False\n",
    "\n",
    "## Time Frame \n",
    "start_date = pd.to_datetime('2014-08-11')\n",
    "end_date = start_date + pd.Timedelta(days=7)\n",
    "forecast_one_cluster['ds'] = pd.to_datetime(forecast_one_cluster['ds'])\n",
    "temp_df = forecast_one_cluster[(forecast_one_cluster['ds'] >= start_date) & (forecast_one_cluster['ds'] <= end_date)]\n",
    "temp_df = temp_df[['ds', 'ID', 'y', 'yhat_day_ahead', 'residuals']]\n",
    "\n",
    "temp_df['MAE'] = temp_df['residuals'].abs()\n",
    "temp_df_agg = temp_df.copy()\n",
    "temp_df_agg['residual_total'] = temp_df_agg['y'] - temp_df_agg['yhat_day_ahead']\n",
    "temp_df_agg = temp_df_agg.groupby('ds').sum().reset_index()\n",
    "temp_df = temp_df.merge(temp_df_agg[['ds', 'residual_total']], on='ds', how='left')\n",
    "temp_df = temp_df.merge(cluster_mapping, on='ID', how='left')\n",
    "temp_df = temp_df.merge(buss_magnitude_aggregated.rename(columns={'new_ID': 'ID'}), on='ID', how='left')\n",
    "magnitude_sort = temp_df.groupby('ID').agg({'relative_magnitude_agg': 'sum'}).sort_values(by='relative_magnitude_agg', ascending=False).reset_index()\n",
    "top_id = magnitude_sort['ID'].head(no_id).tolist()\n",
    "\n",
    "if aggregation_per_cluster:\n",
    "    temp_df = temp_df.drop(columns=['ID'])\n",
    "    temp_df = temp_df.groupby(['ds', 'cluster']).sum().reset_index()\n",
    "    temp_df = temp_df.sort_values(by='relative_magnitude_agg', ascending=False)\n",
    "\n",
    "top_df = temp_df[temp_df['ID'].isin(top_id)]\n",
    "low_df = temp_df[~temp_df['ID'].isin(top_id)]\n",
    "low_df['ID'] = np.where(low_df['residuals'] > 0, 'Other Positive', 'Other Negative')\n",
    "temp_df = pd.concat([top_df, low_df], ignore_index=True)\n",
    "\n",
    "grouped = temp_df.groupby(['ds', 'ID']).residuals.sum().reset_index()\n",
    "pivot_df = grouped.pivot(index='ds', columns='ID', values='residuals').fillna(0)\n",
    "data = pivot_df.values.T\n",
    "data = data.astype(float)\n",
    "data_shape = np.shape(data)\n",
    "\n",
    "def get_cumulated_array(data, **kwargs):\n",
    "    cum = data.clip(**kwargs)\n",
    "    cum = np.cumsum(cum, axis=0)\n",
    "    d = np.zeros(np.shape(data))\n",
    "    d[1:] = cum[:-1]\n",
    "    return d\n",
    "\n",
    "cumulated_data = get_cumulated_array(data, min=0)\n",
    "cumulated_data_neg = get_cumulated_array(data, max=0)\n",
    "row_mask = (data<0)\n",
    "cumulated_data[row_mask] = cumulated_data_neg[row_mask]\n",
    "data_stack = cumulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 5))\n",
    "colors = stanford_colors[:no_id] + ['lightgray', 'lightgray']\n",
    "\n",
    "pivot_df.columns = pivot_df.columns.astype(str)\n",
    "pivot_df.columns = [x if x in ['Other Positive', 'Other Negative'] else 'L-bus ' + chr(ord('A') + i) for i, x in enumerate(pivot_df.columns)]\n",
    "component_labels = pivot_df.columns.tolist()\n",
    "component_labels = ['Other' if x in ['Other Positive', 'Other Negative'] else x for x in component_labels]\n",
    "\n",
    "## Plot bars\n",
    "handles = []\n",
    "labels = []\n",
    "for i, label in enumerate(pivot_df.columns):\n",
    "    bar = ax.bar(pivot_df.index, data[i], bottom=data_stack[i], width=0.03, label=component_labels[i], color=colors[i])\n",
    "    handles.append(bar[0])\n",
    "    labels.append(component_labels[i])\n",
    "\n",
    "## Plot residual line\n",
    "ax.plot(pivot_df.index, pivot_df.sum(axis=1), color='black', linewidth=2, label='Total')\n",
    "handles.insert(0, ax.lines[0])\n",
    "labels.insert(0, 'Total')\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq='1H')\n",
    "custom_ticks = pd.date_range(start_date, end_date, freq='12H')\n",
    "custom_labels = []\n",
    "\n",
    "for tick in custom_ticks:\n",
    "    if tick.hour == 0:\n",
    "        custom_labels.append(tick.strftime('%a'))\n",
    "    else:\n",
    "        custom_labels.append(tick.strftime('%H:%M'))\n",
    "\n",
    "ax.set_xticks(custom_ticks)\n",
    "ax.set_xticklabels(custom_labels)\n",
    "ax.legend(handles = handles[:-1], labels = labels[:-1], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.set_ylabel('Residuals [MW]')\n",
    "ax.set_xlabel('Date')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Bus share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparation\n",
    "\n",
    "temp_save = forecast_test_all_da[['ds', 'ID', 'y', 'yhat_day_ahead']]\n",
    "temp_save['ds'] = pd.to_datetime(temp_save['ds'])\n",
    "temp_save['ID'] = temp_save['ID'].astype(str)\n",
    "temp_save = temp_save.groupby(['ds', 'ID']).sum().reset_index()\n",
    "temp_save = temp_save.merge(cluster_mapping, on='ID', how='left')\n",
    "temp_save['residuals'] = temp_save['y'] - temp_save['yhat_day_ahead']\n",
    "\n",
    "temp_save['MAE'] = temp_save['residuals'].abs()\n",
    "total_error_mae = temp_save[['ID', 'MAE']].groupby('ID').mean().reset_index()\n",
    "total_mae = total_error_mae['MAE'].sum()\n",
    "total_error_mae['MAE_share'] = total_error_mae['MAE'] / total_mae\n",
    "temp_df_agg = temp_save.copy()\n",
    "temp_df_agg = temp_df_agg.groupby(['ds', 'ID']).sum().reset_index()\n",
    "temp_df_agg['residual_total'] = temp_df_agg['y'] - temp_df_agg['yhat_day_ahead']\n",
    "\n",
    "temp_df_agg = temp_df_agg.groupby('ds').sum().reset_index()\n",
    "temp_save = temp_save.merge(temp_df_agg[['ds', 'residual_total']], on='ds', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparation\n",
    "\n",
    "sortierung = 'mae_share'\n",
    "quantile = 0.90\n",
    "\n",
    "## Positive share\n",
    "result_df_pos = temp_save[temp_save['residual_total'] > 0]\n",
    "threshold = result_df_pos['residual_total'].quantile(quantile)\n",
    "result_df_pos = result_df_pos[result_df_pos['residual_total'] > threshold]\n",
    "\n",
    "total_error_pos = result_df_pos.groupby('ID').mean().reset_index()\n",
    "\n",
    "total_error_pos['bias_pos'] = total_error_pos.apply(lambda row: row['residuals'] / row['residual_total'], axis=1)\n",
    "total_error_pos['error_pos'] = total_error_pos.apply(lambda row: row['MAE'] / row['residual_total'], axis=1)\n",
    "\n",
    "## Negative share\n",
    "temp_save['residual_total'] = temp_save['residual_total'] * -1\n",
    "result_df_neg = temp_save[temp_save['residual_total'] > 0]\n",
    "threshold = result_df_neg['residual_total'].quantile(quantile)\n",
    "result_df_neg = result_df_neg[result_df_neg['residual_total'] > threshold]\n",
    "temp_save['residual_total'] = temp_save['residual_total'] * -1\n",
    "\n",
    "total_error_neg = result_df_neg.groupby('ID').mean().reset_index()\n",
    "\n",
    "total_error_neg['bias_neg'] = total_error_neg.apply(lambda row: row['residuals'] / row['residual_total'], axis=1)\n",
    "total_error_neg['bias_neg'] = total_error_neg['bias_neg'] * (-1)\n",
    "total_error_neg['error_neg'] = total_error_neg.apply(lambda row: row['MAE'] / row['residual_total'], axis=1)\n",
    "\n",
    "## Merge all together\n",
    "total_error = total_error_pos[['ID', 'bias_pos', 'error_pos']].merge(total_error_neg[['ID', 'bias_neg', 'error_neg']], on='ID', how='left')\n",
    "total_error = total_error.merge(total_error_mae[['ID', 'MAE_share']], on='ID', how='left')\n",
    "\n",
    "## Change bus names \n",
    "total_error = total_error.merge(cluster_mapping, on='ID', how='left')\n",
    "total_error = total_error.merge(buss_magnitude_aggregated.rename(columns={'new_ID': 'ID'}), on='ID', how='left')\n",
    "total_error = total_error.sort_values(by='relative_magnitude_agg', ascending=False).reset_index(drop=True)\n",
    "total_error['rank'] = total_error.groupby('cluster')['relative_magnitude_agg'].rank(ascending=False, method='first').astype(int)\n",
    "total_error['ID_names'] = 'L-bus ' + total_error['rank'].astype(str).apply(lambda x: chr(64 + int(x))) + ' - Cluster ' + total_error['cluster'].astype(str)\n",
    "total_error_before_sorting = total_error.copy()\n",
    "\n",
    "if sortierung == 'bias_pos':\n",
    "    total_error = total_error.sort_values(by='bias_pos', ascending=False)\n",
    "if sortierung == 'bias_neg':\n",
    "    total_error = total_error.sort_values(by='bias_neg', ascending=False)\n",
    "if sortierung == 'mae_share':\n",
    "    total_error = total_error.sort_values(by='MAE_share', ascending=False)\n",
    "\n",
    "## Extract the top rows\n",
    "total_error = total_error.head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "bias = 'black'\n",
    "error = 'lightgray'\n",
    "points = 'gray'\n",
    "\n",
    "fig = plt.figure(figsize=(17, 4))\n",
    "gs = gridspec.GridSpec(1, 4, width_ratios=[0.9, 1.2, 1.2, 0.9], wspace=0.95)\n",
    "\n",
    "## Subplot 1: horizontal bar plot with relative share of RES\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax1.barh(total_error['ID_names'], total_error['MAE_share'], color='none', edgecolor=points, label='Relative Aggregated Residuals')\n",
    "ax1.set_xlabel('Overall MAE Share')\n",
    "ax1.set_yticks([])\n",
    "ax1.tick_params(axis=u'both', which=u'both', length=0)\n",
    "ax1.set_xlim(0, 0.025)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "## Subplot 2: Vertical line at x=0 and negative bar plot\n",
    "ax2 = plt.subplot(gs[1])\n",
    "ax2.barh(total_error['ID_names'], total_error['error_neg'], label='Mean Absolute Error', color=error)\n",
    "ax2.barh(total_error['ID_names'], total_error['bias_neg'], label='Mean Error (Bias)', color=bias)\n",
    "ax2.set_xlabel(f'Relative Contribution to {round(100-quantile*100)} Percent \\n Highest Negative Residuals')\n",
    "ax2.set_yticks([])\n",
    "ax2.invert_xaxis()\n",
    "ax2.grid(True, axis='x', color='lightgray')\n",
    "ax2.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.legend(loc='upper left', bbox_to_anchor=(-0.75, 1.0))\n",
    "\n",
    "## Subplot 3: Horizontal bar plot with positive RES\n",
    "ax3 = plt.subplot(gs[2])\n",
    "ax3.barh(total_error['ID_names'], total_error['error_pos'], label='Mean Absolute Error', color=error)\n",
    "ax3.barh(total_error['ID_names'], total_error['bias_pos'], label='Mean Error (Bias)', color=bias)\n",
    "ax3.set_xlabel(f'Relative Contribution to {round(100-quantile*100)} Percent \\n Highest Positive Residuals')\n",
    "ax3.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax3.grid(True, axis='x', color='lightgray')\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.spines['bottom'].set_visible(False)\n",
    "ax3.spines['left'].set_visible(False)\n",
    "ax3.legend(loc='upper right', bbox_to_anchor=(1.75, 1.0))\n",
    "\n",
    "## Subplot 4: Horizontal bar plot with magnitude\n",
    "ax4 = plt.subplot(gs[3])\n",
    "ax4.barh(total_error['ID_names'], total_error['relative_magnitude_agg'], color='none', edgecolor=points, label='Relative Magnitude')\n",
    "ax4.set_xlabel('Overall Load Share')\n",
    "ax4.set_yticks([])\n",
    "ax4.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax4.set_xlim(0, 0.02)\n",
    "ax4.spines['top'].set_visible(False)\n",
    "ax4.spines['right'].set_visible(False)\n",
    "ax4.spines['bottom'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Aggregate Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the absolute magnitude for pre- & post aggregation\n",
    "\n",
    "buss_magnitude_abs = pd.DataFrame()\n",
    "buss_magnitude_aggregated_abs = pd.DataFrame()\n",
    "buss_train_mean = buss_train[['ID', 'y']].groupby('ID').mean().reset_index()\n",
    "buss_train_mean['y'] = buss_train_mean['y'].abs()\n",
    "buss_train_mean = buss_train_mean.rename(columns={'y': 'y_mean'})\n",
    "buss_magnitude_abs = buss_train_mean \n",
    "buss_magnitude_aggregated_abs = pd.DataFrame()\n",
    "helper = []\n",
    "\n",
    "for index, row in mapping.iterrows():\n",
    "    ids = row['ID_list'].split(';')\n",
    "    new_id = row['new_ID']\n",
    "\n",
    "    filtered_rows = buss_train_mean[buss_train_mean['ID'].isin(ids)]\n",
    "    magnitude = filtered_rows['y_mean'].sum()\n",
    "    tempo = pd.DataFrame({'new_ID': [new_id], 'magnitude_agg': [magnitude]})\n",
    "    helper.append(tempo)\n",
    "\n",
    "buss_magnitude_aggregated_abs = pd.concat(helper).reset_index(drop=True)\n",
    "temp = forecast_test_all_da[~forecast_test_all_da['ID'].isin(buss_magnitude_aggregated_abs['new_ID'])]\n",
    "temp = temp['ID'].unique()\n",
    "temp = buss_magnitude_abs[buss_magnitude_abs['ID'].isin(temp)]\n",
    "not_agg = temp[['ID', 'y_mean']]\n",
    "buss_magnitude_aggregated_abs = pd.concat([buss_magnitude_aggregated_abs, not_agg.rename(columns={'ID': 'new_ID', 'y_mean': 'magnitude_agg'})]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "axs[0].hist(buss_magnitude_abs['y_mean'], bins=200, color='grey')\n",
    "axs[0].set_ylabel('Number of Buses')\n",
    "\n",
    "axs[1].hist(buss_magnitude_aggregated_abs['magnitude_agg'], bins=100, color='grey')\n",
    "axs[1].set_ylabel('Number of L-Buses')\n",
    "axs[1].set_xlabel('Average Load [MW]')\n",
    "axs[1].set_xscale('linear') \n",
    "axs[0].set_xlim(0, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
