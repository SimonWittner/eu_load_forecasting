{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from utils.data_processing import _drop_consecutive_nans, add_day_ahead_column\n",
    "from utils.error_metrics import _calc_mae, _calc_mse, _calc_rmse, _calc_nrmse, _calc_mape, _calc_mase, _calc_msse, _seas_naive_fcst, _calc_metrics\n",
    "from utils.clustering import mapping_tsfeatures, clustering, sum_until_threshold, mapping_energy_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load csv - 1min\n",
    "\n",
    "df = pd.read_csv(\"01_load_bus.csv\")\n",
    "unique_id = df[\"ID\"].unique()\n",
    "df['ID'] = df['ID'].astype(str)\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "print(\"Number of unique ID: \", len(unique_id))\n",
    "\n",
    "### Mapping\n",
    "mapping = pd.read_csv(\"02_mapping.csv\")\n",
    "mapping['ID'] = mapping['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop all rows with no information (y=0)\n",
    "\n",
    "zeros = df[df[\"y\"] == 0]\n",
    "id_list = zeros['ID'].unique()\n",
    "print(\"Number of ID with y=0: \", len(id_list))\n",
    "\n",
    "\n",
    "df = df[~df['ID'].isin(id_list)]\n",
    "print('Number of unique ID after 0s: ', df['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Empirical Features - using tsfeatures - takes a while\n",
    "\n",
    "calculate_new = False\n",
    "\n",
    "if calculate_new:\n",
    "    temp = df[['ds', 'ID', 'y']]\n",
    "    temp['y'] = temp['y'].astype('float32')\n",
    "\n",
    "    features = mapping_tsfeatures(df=temp, normalise=True, freq=24, calulate_tsfeatures=True, calculate_em=True)\n",
    "\n",
    "    features.to_csv(\"02_features_bus.csv\", index=False)\n",
    "else:\n",
    "    features = pd.read_csv(\"02_features_bus.csv\")\n",
    "\n",
    "features['ID'] = features['ID'].astype(str)\n",
    "features = features.dropna()\n",
    "df_pre_clu = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering - using an 'elbow' method to determine the number of clusters\n",
    "\n",
    "n_cluster = 40\n",
    "df_cluster = df[[\"ID\", \"ds\", \"y\"]]\n",
    "\n",
    "clusters = clustering(\n",
    "    ID_feature=features,\n",
    "    df=df_cluster,\n",
    "    n_cluster=n_cluster,\n",
    "    normalise=True,\n",
    "    n_pca=1,\n",
    "    elbow=25,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mean Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mean magnitude per ID \n",
    "\n",
    "df_magnitude = df.groupby(\"ID\").mean().reset_index()\n",
    "\n",
    "df_magnitude['ID'] = df_magnitude['ID'].astype(str)\n",
    "clusters['ID'] = clusters['ID'].astype(str)\n",
    "\n",
    "df_magnitude = df_magnitude.rename(columns={\"y\": \"y_mean\"})\n",
    "df_magnitude = pd.merge(df_magnitude, mapping[[\"ID\", \"country\"]], on=\"ID\")\n",
    "df_magnitude = pd.merge(df_magnitude, clusters[[\"ID\", \"cluster\"]], on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Aggregation of Bus-Nodes (Mid-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge df with df_magntiude[cluster] on ID\n",
    "\n",
    "df = pd.merge(df, df_magnitude[[\"ID\", \"cluster\"]], on=\"ID\")\n",
    "df = pd.merge(df, mapping[[\"ID\", \"country\"]], on=\"ID\")\n",
    "df_magnitude['y_mean'] = df_magnitude['y_mean'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregation - min\n",
    "\n",
    "## Number of rounds (aggregations) per country & cluster - will be interrupted once only 1 ID is left\n",
    "agg_no = 60\n",
    "\n",
    "## Choose Threshold\n",
    "# Local Threshold - Quantile for each country & cluster\n",
    "threshold_value = 0.85\n",
    "threshold_string = f\"local_threshold_q{threshold_value}\"\n",
    "\n",
    "# Cluster Threshold - Quantile for each cluster\n",
    "threshold_cluster = False\n",
    "if threshold_cluster:\n",
    "    quantile = 0.9\n",
    "    threshold_string = f\"cluster_threshold_q{quantile}\"\n",
    "    threshold_value = df_magnitude.groupby(\"cluster\")[\"y_mean\"].quantile(quantile).reset_index()\n",
    "    threshold_value.columns = [\"cluster\", \"threshold\"]\n",
    "\n",
    "# country Threshold - Quantile for each country\n",
    "threshold_country = True\n",
    "if threshold_country:\n",
    "    quantile = 0.90\n",
    "    threshold_string = f\"country_threshold_q{quantile}\"\n",
    "    threshold_value = df_magnitude.groupby(\"country\")[\"y_mean\"].quantile(quantile).reset_index()\n",
    "    threshold_value.columns = [\"country\", \"threshold\"]\n",
    "\n",
    "# Global Threshold\n",
    "threshold_global = False\n",
    "if threshold_global:\n",
    "    quantile = 0.9\n",
    "    threshold_string = f\"global_threshold_q{quantile}\"\n",
    "    threshold_value = df_magnitude['y_mean'].quantile(quantile)\n",
    "\n",
    "## Aggregation\n",
    "ID_to_add, df_after, mapping = sum_until_threshold(\n",
    "    df=df[['ID', 'ds', 'y', 'country', 'cluster']],\n",
    "    df_ID=df_magnitude[['ID', 'y_mean', 'cluster', 'country']],\n",
    "    threshold_value=threshold_value,\n",
    "    threshold_global=threshold_global,\n",
    "    threshold_cluster=threshold_cluster,\n",
    "    threshold_country=threshold_country,\n",
    "    agg_no=agg_no\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "\n",
    "df_after_test = df_after.drop_duplicates(subset=[\"ID\", \"cluster\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of unique ID in df_after for each cluster after aggregation:\\n\", df_after_test[\"cluster\"].value_counts())\n",
    "print('\\nTotal number of unique ID in df_after:', df_after_test['ID'].nunique())\n",
    "\n",
    "df_after.to_csv(\"03_df_after.csv\")\n",
    "df_after = df_after.rename(columns={\"cluster\": \"cluster_after_aggregation\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Second Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate new ts features after aggregation - using tsfeatures - takes a while\n",
    "\n",
    "calculate_new = False\n",
    "\n",
    "if calculate_new:\n",
    "    ### calculate ts & em features for df_after\n",
    "    df_temp = df_after[[\"ID\", \"ds\", \"y\"]]\n",
    "    df_temp['y'] = df_temp['y'].astype('float32')\n",
    "    \n",
    "    df_after_features = mapping_tsfeatures(df=df_temp)\n",
    "\n",
    "    # save df_after_features\n",
    "    df_after_features.to_csv(\"04_features_after_aggregation.csv\")\n",
    "else:\n",
    "    df_after_features = pd.read_csv(\"04_features_after_aggregation.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2nd Clustering\n",
    "\n",
    "n_cluster = 10\n",
    "df_cluster = df_after[[\"ID\", \"ds\", \"y\"]]\n",
    "\n",
    "clusters = clustering(\n",
    "    ID_feature=df_after_features,\n",
    "    df=df_cluster,\n",
    "    n_cluster=n_cluster,\n",
    "    normalise=True,\n",
    "    n_pca=1,\n",
    "    elbow=35,\n",
    "    )\n",
    "\n",
    "clusters['ID'] = clusters['ID'].astype(str)\n",
    "df_after['ID'] = df_after['ID'].astype(str)\n",
    "\n",
    "# merge df_after with clusters\n",
    "df_after = pd.merge(df_after, clusters[[\"ID\", \"cluster\"]], on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Potential Subclustering - Optional!\n",
    "\n",
    "# Which cluster?\n",
    "no = 3\n",
    "# How many subclusters?\n",
    "n_cluster_sub = 3\n",
    "# Sharp or not?\n",
    "sharp = False\n",
    "\n",
    "filtered_rows = clusters[clusters['cluster'] == no]\n",
    "filtered_rows = filtered_rows.drop(columns=[\"cluster\"])\n",
    "\n",
    "clusters_sub = pd.DataFrame()\n",
    "\n",
    "## Subclustering for cluster no\n",
    "print(f\"SUBCLUSTERING FOR CLUSTER {no}\")\n",
    "clusters_sub = clustering(\n",
    "    ID_feature=filtered_rows,\n",
    "    df=df_cluster,\n",
    "    n_cluster=n_cluster_sub,\n",
    "    normalise=True,\n",
    "    n_pca=1,\n",
    "    elbow=25,\n",
    "    )\n",
    "\n",
    "clusters_sub = clusters_sub.rename(columns={\"cluster\": \"cluster_sub\"})\n",
    "clusters_sub = clusters_sub[[\"ID\", \"cluster_sub\"]].reset_index(drop=True)\n",
    "name = f\"cluster_sub_{no}\"\n",
    "clusters_sub = clusters_sub.rename(columns={\"cluster_sub\": name})\n",
    "clusters_sub[\"cluster_after_sub\"] = df_after[\"cluster\"].max() + clusters_sub[name] + 1\n",
    "\n",
    "\n",
    "## df_after\n",
    "df_after_temp = df_after[df_after[\"cluster\"] == no]\n",
    "\n",
    "if sharp:\n",
    "    df_after = df_after[df_after[\"cluster\"] != no]\n",
    "\n",
    "df_after_temp = pd.merge(df_after_temp, clusters_sub[['ID', name, 'cluster_after_sub']], on=\"ID\")\n",
    "df_after_temp = df_after_temp.drop(columns=[\"cluster\"])\n",
    "df_after_temp = df_after_temp.rename(columns={\"cluster_after_sub\": \"cluster\"})\n",
    "\n",
    "\n",
    "if sharp:\n",
    "    df_after = pd.concat([df_after, df_after_temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "df_after_test = df_after.drop_duplicates(subset=[\"ID\", \"cluster\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of unique ID in df_after for each cluster after aggregation:\\n\", df_after_test[\"cluster\"].value_counts())\n",
    "print('\\nTotal number of unique ID in df_after:', df_after_test['ID'].nunique())\n",
    "\n",
    "df_after.to_csv(\"05_df_after_95per_subclustering.csv\")\n",
    "mapping.to_csv(\"06_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = pd.read_csv(\"01_load_bus.csv\")\n",
    "df = df.drop(columns=[\"cluster_after_aggregation\"])\n",
    "\n",
    "temperature['ID'] = temperature['ID'].astype(str)\n",
    "temperature['ds'] = pd.to_datetime(temperature['ds'])\n",
    "\n",
    "### Mapping\n",
    "country_mapping = pd.read_csv('network_nodes.csv')\n",
    "country_mapping['ID'] = country_mapping['ID'].astype(str)\n",
    "\n",
    "temperature = pd.merge(temperature, country_mapping[[\"ID\", \"country\"]], on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mean temeprature per country\n",
    "\n",
    "temperature = temperature[['ds', 'country', 't2m']].groupby([\"ds\", \"country\"]).mean().reset_index()\n",
    "temperature['ds'] = pd.to_datetime(temperature['ds'])\n",
    "temperature['country'] = temperature['country'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge with Average Country Temperature\n",
    "\n",
    "df_temp = df.copy()\n",
    "df_temp = df_temp.rename(columns={\"country\": \"country\"})\n",
    "df_temp[\"ds\"] = pd.to_datetime(df_temp[\"ds\"])\n",
    "df_temp['country'] = df_temp['country'].astype(str)\n",
    "df_temp = pd.merge(df_temp, temperature, on=[\"country\", \"ds\"])\n",
    "df = df_temp[['ID', 'ds', 'y', 't2m', 'country', 'cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Clusters of df\n",
    "for j in df[\"cluster\"].unique():\n",
    "    df_cluster = df[df[\"cluster\"] == j]\n",
    "    df_cluster.to_csv(f\"0{j}_load_bus_after.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
