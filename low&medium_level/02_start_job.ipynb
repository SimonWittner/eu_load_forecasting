{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At this point the ./src/main.py file needs to be executed with the preprocessed data. In the original training, Azure ML Studio was used to train the model. If you use another cloud computing service, you will need to adapt the code to your needs.\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "import sys \n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "## Change according to your workspace\n",
    "from config import subscription_id, resource_group, workspace_name\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group_name = resource_group,\n",
    "    workspace_name = workspace_name\n",
    ")\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "datastore = Datastore.get(workspace, \"workspaceblobstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set up job environment (only for first training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)\n",
    "\n",
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy\n",
    "  - pip\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - mlflow>=1.26.1\n",
    "    - azureml-mlflow==1.42.0\n",
    "    - psutil>=5.8,<5.9\n",
    "    - tqdm>=4.59,<4.60\n",
    "    - ipykernel~=6.0\n",
    "    - git+https://github.com/ourownstory/neural_prophet.git\n",
    "    - pytorch-lightning>=1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"training_env\"\n",
    "\n",
    "custom_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Virtual environment for NP training\",\n",
    "    tags={\"neuralprophet\": \"main github\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "custom_job_env = ml_client.environments.create_or_update(custom_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {custom_job_env.name} is registered to workspace, the environment version is {custom_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path to input file on Azure ML\n",
    "\n",
    "data_path= \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/09_load_bus_after.csv\"\n",
    "filename_wcsv = os.path.basename(data_path)\n",
    "filename, _ = os.path.splitext(filename_wcsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Start training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create your base command job\n",
    "\n",
    "job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"\"\" python main.py \\\n",
    "            --data ${{inputs.data}} \\\n",
    "            --results ${{outputs.results}} \\\n",
    "            \"\"\",\n",
    "    environment=\"training_env@latest\",\n",
    "    inputs=dict(\n",
    "        data=Input(\n",
    "            type=\"uri_file\",\n",
    "            path=data_path\n",
    "        ),\n",
    "    ),\n",
    "    outputs=dict(\n",
    "        results=Output(type=\"uri_folder\", \n",
    "                     mode=\"rw_mount\"),\n",
    "    ),\n",
    "    compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
    "    display_name=filename,\n",
    "    experiment_name=\"Clustering_bus\",\n",
    "    description=\"Clustered & aggregated Bus\",\n",
    "    name=f'{filename}',\n",
    ")\n",
    "\n",
    "# Submit the job\n",
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List with all cluster dfs\n",
    "\n",
    "list_uri = [\"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/00_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/01_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/02_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/03_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/04_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/05_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/06_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/07_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/08_load_bus_after.csv\",\n",
    "            \"azureml://subscriptions/resourcegroups/datastores/workspaceblobstore/09_load_bus_after.csv\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start all cluster-jobs\n",
    "\n",
    "for uri in list_uri:\n",
    "    filename_wcsv = os.path.basename(uri)\n",
    "    filename, _ = os.path.splitext(filename_wcsv)\n",
    "    print(filename)\n",
    "\n",
    "    job = command(\n",
    "        code=\"./src\",\n",
    "        command=\"\"\" python main.py \\\n",
    "                --data ${{inputs.data}} \\\n",
    "                --results ${{outputs.results}} \\\n",
    "                \"\"\",\n",
    "        environment=\"training_env@latest\",\n",
    "        inputs=dict(\n",
    "            data=Input(\n",
    "                type=\"uri_file\",\n",
    "                path=uri\n",
    "            ),\n",
    "        ),\n",
    "        outputs=dict(\n",
    "            results=Output(type=\"uri_folder\", \n",
    "                        mode=\"rw_mount\"),\n",
    "        ),\n",
    "        compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
    "        display_name=filename,\n",
    "        experiment_name=\"Clustering_bus\",\n",
    "        description=\"Clustered & aggregated Bus\",\n",
    "        name=f'{filename}_v5',\n",
    "    )\n",
    "    \n",
    "    # submit the job\n",
    "    ml_client.create_or_update(job)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
