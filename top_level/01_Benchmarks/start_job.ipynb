{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697319454850
        }
      },
      "outputs": [],
      "source": [
        "### At this point the ./src/main.py file needs to be executed with the preprocessed data. In the original training, Azure ML Studio was used to train the model. If you use another cloud computing service, you will need to adapt the code to your needs.\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml import command, Input, Output\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\n",
        "from azureml.core import Workspace,Dataset, Datastore, Experiment\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../../../\")\n",
        "\n",
        "# Get a handle to the workspace\n",
        "credential = DefaultAzureCredential()\n",
        "from config import subscription_id, resource_group, workspace_name\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id = subscription_id,\n",
        "    resource_group_name = resource_group,\n",
        "    workspace_name = workspace_name\n",
        ")\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "datastore = Datastore.get(workspace, \"workspaceblobstore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Set up job environment (only first time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: benchmark-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.10\n",
        "  - numpy\n",
        "  - pandas\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - mlflow== 2.4.1\n",
        "    - azureml-mlflow==1.51.0\n",
        "    - azureml-core==1.53.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - ipykernel~=6.0\n",
        "    - u8darts[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"benchmark_env\"\n",
        "\n",
        "custom_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Virtual environment for Benchmarking\",\n",
        "    tags={\"additional\": \"darts\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        ")\n",
        "custom_job_env = ml_client.environments.create_or_update(custom_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {custom_job_env.name} is registered to workspace, the environment version is {custom_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set up input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# path to input file\n",
        "data_path= \"azureml://subscriptions/workspaceblobstore/paths/LocalUpload/00_load_country.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# validate the input data\n",
        "short_path = 'LocalUpload' + data_path.split('LocalUpload')[1] \n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, short_path))\n",
        "df = dataset.to_pandas_dataframe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create your base command job\n",
        "command_job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"\"\" python xgboost_hyperparam.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --learning_rate ${{inputs.learning_rate}} \\\n",
        "            --subsample ${{inputs.subsample}} \\\n",
        "            --max_leaves ${{inputs.max_leaves}} \\\n",
        "            --max_depth ${{inputs.max_depth}} \\\n",
        "            --gamma ${{inputs.gamma}} \\\n",
        "            --colsample_bytree ${{inputs.colsample_bytree}} \\\n",
        "            --min_child_weight ${{inputs.min_child_weight}} \\\n",
        "            --results ${{outputs.results}} \\\n",
        "            \"\"\",\n",
        "    environment=\"benchmark_env@latest\",\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=data_path\n",
        "        ),\n",
        "        #max_depth = 3,\n",
        "        #gamma = 1.0,\n",
        "        #reg_alpha = 40,\n",
        "        #reg_lambda = 0.0,\n",
        "        #colsample_bytree = 0.5,\n",
        "        #min_child_weight = 0,\n",
        "        #n_estimators = 180,\n",
        "        #seed = 0,\n",
        "        learning_rate = 0.1,\n",
        "        subsample = 1,\n",
        "        max_leaves = 100,\n",
        "        max_depth = 5,\n",
        "        gamma = 0,\n",
        "        colsample_bytree = 1,\n",
        "        min_child_weight = 1,\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        results=Output(type=\"uri_folder\", \n",
        "                     mode=\"rw_mount\"),\n",
        "    ),\n",
        "    compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
        ")\n",
        "\n",
        "# override inputs with hyperparameter values\n",
        "#command_job_for_sweep = command_job(\n",
        "#    max_depth=Choice(values=range(3, 19, 1)), #maximum depth of a tree\n",
        "#    gamma=Uniform(1.0, 9.0), #minimum loss reduction required to make a further partition on a leaf node of the tree\n",
        "#    reg_alpha=Choice(values=range(40, 181, 1)), #L1 regularization term on weights\n",
        "#    reg_lambda=Uniform(0.0, 1.0), #L2 regularization term on weights\n",
        "#    colsample_bytree=Uniform(0.5, 1.0), #fraction of columns to be randomly sampled for each tree\n",
        "#    min_child_weight=Choice(values=range(0, 11, 1)), #minimum sum of instance weight (hessian) needed in a child\n",
        "#    n_estimators=180, #number of trees\n",
        "#    seed=0, #random seed\n",
        "#)\n",
        "\n",
        "command_job_for_sweep = command_job(\n",
        "    learning_rate=Uniform(0.005, 0.2),\n",
        "    subsample=Uniform(0.8, 1),\n",
        "    max_leaves=Choice(values=range(10, 201, 10)),\n",
        "    max_depth=Choice(values=range(5, 31, 5)),\n",
        "    gamma=Uniform(0, 0.02),\n",
        "    colsample_bytree=Uniform(0.8, 1),\n",
        "    min_child_weight=Choice(values=range(0, 11, 1)),\n",
        ")\n",
        "\n",
        "# Call sweep() on your command job to sweep over your parameter expressions\n",
        "sweep_job = command_job_for_sweep.sweep(\n",
        "    compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
        "    sampling_algorithm=\"bayesian\",\n",
        "    primary_metric=\"MASE\",\n",
        "    goal=\"Minimize\",\n",
        ")\n",
        "\n",
        "#Specify your experiment details\n",
        "sweep_job.display_name = \"Bayesian hyperparameter tuning for XGBoost\"\n",
        "sweep_job.experiment_name = \"0821_Benchmarking\"\n",
        "sweep_job.description = \"Hyperparameter tuning for XGBoost using Bayesian sampling and Azure ML\"\n",
        "\n",
        "# Define the limits for this sweep\n",
        "sweep_job.set_limits(max_total_trials=100, max_concurrent_trials=10, timeout=10*60*60)\n",
        "\n",
        "# Set early stopping on this one\n",
        "sweep_job.early_termination = MedianStoppingPolicy(\n",
        "    delay_evaluation=5, evaluation_interval=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# submit the sweep\n",
        "returned_sweep_job = ml_client.create_or_update(sweep_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# best parameters\n",
        "best_params = {\n",
        "    \"learning_rate\": 0.123,\n",
        "    \"subsample\": 0.861,\n",
        "    \"max_leaves\": 20,\n",
        "    \"max_depth\": 30,\n",
        "    \"gamma\": 0.00039,\n",
        "    \"colsample_bytree\": 0.874,\n",
        "    \"min_child_weight\": 7,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697319469704
        }
      },
      "outputs": [],
      "source": [
        "# Create your base command job\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"\"\" python xgboost_main.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --results ${{outputs.results}} \\\n",
        "            \"\"\",\n",
        "    # use environment version 12\n",
        "    environment=\"benchmark_env:20\",\n",
        "    #environment=\"benchmark_env@latest\",\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=data_path\n",
        "        ),\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        results=Output(type=\"uri_folder\", \n",
        "                     mode=\"rw_mount\"),\n",
        "    ),\n",
        "    compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
        "    #compute=\"Standard-NC24ads-A100-v4\",\n",
        "    #compute=\"Standard-NC6\",\n",
        "    #compute='leoniew-StandardNC6',\n",
        "    #compute=\"Standard-HB120rs-v3\",\n",
        "    display_name=f\"Full model xgboost\",\n",
        "    experiment_name=\"0821_Benchmarking\",\n",
        "    description=\"Benchmark country data\",\n",
        ")\n",
        "\n",
        "# submit the job\n",
        "ml_client.create_or_update(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Start ARIMA job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1697319475616
        }
      },
      "outputs": [],
      "source": [
        "IDs = df['country'].unique()\n",
        "IDs = ['DEU']\n",
        "#for id in IDs[8:]:\n",
        "for id in IDs:\n",
        "    # Create your base command job\n",
        "    job = command(\n",
        "        code=\"./src\",\n",
        "        command=\"\"\" python arima_main.py \\\n",
        "                --data ${{inputs.data}} --id ${{inputs.id}} \\\n",
        "                --results ${{outputs.results}} \\\n",
        "                \"\"\",\n",
        "        # use environment version 12\n",
        "        environment=\"benchmark_env:20\",\n",
        "        #environment=\"benchmark_env@latest\",\n",
        "        inputs=dict(\n",
        "            data=Input(\n",
        "                type=\"uri_file\",\n",
        "                path=data_path\n",
        "            ),\n",
        "            id=id,\n",
        "        ),\n",
        "        outputs=dict(\n",
        "            results=Output(type=\"uri_folder\", \n",
        "                        mode=\"rw_mount\"),\n",
        "        ),\n",
        "        compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
        "        #compute=\"Standard-NC6\",\n",
        "        #compute='leoniew-StandardNC6',\n",
        "        #compute=\"Standard-HB120rs-v3\",\n",
        "        display_name=f\"Full model arima for {id}\",\n",
        "        experiment_name=\"0821_Benchmarking\",\n",
        "        description=\"Benchmark country data\",\n",
        "        name=f\"arima_{id}_1630\"\n",
        "    )\n",
        "\n",
        "    # submit the job\n",
        "    ml_client.create_or_update(job)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
