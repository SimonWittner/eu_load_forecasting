{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1692922567330
        }
      },
      "outputs": [],
      "source": [
        "### At this point the ./src/main.py file needs to be executed with the preprocessed data. In the original training, Azure ML Studio was used to train the model. If you use another cloud computing service, you will need to adapt the code to your needs.\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml import command, Input, Output\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azureml.core import Workspace, Dataset, Datastore\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "import sys \n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "from config import subscription_id, resource_group, workspace_name\n",
        "\n",
        "ml_client = MLClient( \n",
        "    credential=credential,\n",
        "    subscription_id = subscription_id,\n",
        "    resource_group_name = resource_group,\n",
        "    workspace_name = workspace_name\n",
        ")\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "datastore = Datastore.get(workspace, \"workspaceblobstore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Set up job environment (only first time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {dependencies_dir}/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy\n",
        "  - pip\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - mlflow>=1.26.1\n",
        "    - azureml-mlflow==1.42.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - tqdm>=4.59,<4.60\n",
        "    - ipykernel~=6.0 \n",
        "    - neuralprophet==1.0.0rc5\n",
        "    #- git+https://github.com/ourownstory/neural_prophet.git\n",
        "    #- git=https://github.com/Azure/azhpc-images/blob/master/ubuntu/ubuntu-22.x/ubuntu-22.04-hpc/install.sh\n",
        "    - pytorch-lightning>=1.4.0\n",
        "    - memory_profiler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set up input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683213134470
        }
      },
      "outputs": [],
      "source": [
        "### Input data\n",
        "\n",
        "data_path= 'azureml://subscriptions/workspaceblobstore/paths/LocalUpload/00_load_country.csv'\n",
        "\n",
        "short_path = 'LocalUpload' + data_path.split('LocalUpload')[1] \n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, short_path))\n",
        "df = dataset.to_pandas_dataframe()\n",
        "\n",
        "df['humidity'] = df['humidity'] / 100\n",
        "df['precipitation'] = (df['precipitation'] - df['precipitation'].min()) / (df['precipitation'].max() - df['precipitation'].min())\n",
        "df['cloud'] = df['cloud'] / 100\n",
        "df['wind'] = (df['wind'] - df['wind'].min()) / (df['wind'].max() - df['wind'].min())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Start training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1683213134668
        }
      },
      "outputs": [],
      "source": [
        "### Create your base command job\n",
        "\n",
        "job = command( \n",
        "    code=\"./src\",\n",
        "    command=\"\"\" python main.py \\\n",
        "            --data ${{inputs.data}} \\\n",
        "            --results ${{outputs.results}} \\\n",
        "            \"\"\",\n",
        "    environment=\"training_env@latest\",\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=data_path\n",
        "        ),\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        results=Output(type=\"uri_folder\", \n",
        "                     mode=\"rw_mount\"),\n",
        "    ),\n",
        "    compute=\"Standard-NC24ads-A100-v4-10nodes\",\n",
        "    display_name=f\"country_forecast_2y/1y-5lagreg_v2\",\n",
        "    experiment_name=\"eu\",\n",
        "    description=\"Training NeuralProphet\"\n",
        ")\n",
        "\n",
        "## Submit the job\n",
        "ml_client.create_or_update(job)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
